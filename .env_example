AWS_CONN_ID="aws_default"
S3_BUCKET="<your bucket name>"
INGEST_FOLDER_NAME="<your folder name>"  # the name of the folder you'd like to create inside your S3 Bucket
DBX_CONN_ID="databricks_default"
DBX_WH_HTTP_PATH="/sql/1.0/warehouses/<your endpoint>"
S3_BUCKET_ACCESS_ROLE_ARN="arn:aws:iam::<acct-id>:role/<role-id>"  # the ARN of a Iam role that has permissions to access your S3 Bucket and to create temporary credentials.
DATABRICKS_FOLDER="<your databricks folder>"  # part of the path to your DBX notebooks. If you are using the default location for your user, this will be your DBX signup email.

AIRFLOW_CONN_DATABRICKS_DEFAULT='{
    "conn_type": "databricks",
    "password": "<your token>",
    "host": "https://<your host>.cloud.databricks.com/"
}'
# Databricks host address format: https://dbc-1234cb56-d7c8.cloud.databricks.com/
# For documentation on how to get a Databricks personal access token see: https://docs.databricks.com/dev-tools/auth.html#databricks-personal-access-tokens

AIRFLOW_CONN_AWS_DEFAULT='{
    "conn_type": "aws",
    "login": "<aws access key>",
    "password": "<aws secret key>"
}'
# Documentation on AWS Access Keys: https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html. 


# note that the demo uses a mocked SnowflakeOperator, you do not need valid credentials
SNOWFLAKE_CONN_ID="snowflake_default"
AIRFLOW_CONN_SNOWFLAKE_DEFAULT='{ 
    "conn_type":"snowflake",
    "login":"<your login>",
    "password":"<your password>",
    "schema":"<your schema>",
    "extra":
        {
            "account":"<your account>",
            "warehouse":"<your wh>",
            "database":"<your database>",
            "region":"<your region>",
            "role":"<your role>",
            "authenticator":"snowflake",
            "session_parameters":null,
            "application":"AIRFLOW"
        }
    }'