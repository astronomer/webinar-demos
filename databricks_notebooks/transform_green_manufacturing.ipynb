{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ec7aac6-8474-4f4e-8e44-e8b65acdc3c5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Compute additional values\n",
    "\n",
    "1. Average all green manufacturing across all Facilities for each day\n",
    "2. Compute net power use\n",
    "3. Compute efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dfea71c-471b-4813-8a04-6ae98ca729f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM hive_metastore.default.greenmanufacturing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5022870-1864-45a3-b4d0-b40918d7af49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg\n",
    "\n",
    "\n",
    "avg_df = df.groupBy(\"TS\").agg(\n",
    "    avg(col(\"AmtPowerUse\")).alias(\"AvgAmtPowerUse\"),\n",
    "    avg(col(\"AmtPowerReturnedToGrid\")).alias(\"AvgAmtPowerReturnedToGrid\"),\n",
    "    avg(col(\"AmtProductOutput\")).alias(\"AvgAmtProductOutput\")\n",
    ")\n",
    "\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf89980-0d50-4081-8f9a-b717083bc58b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "net_power_df = avg_df.withColumn(\n",
    "    \"AvgAmtNetPowerConsumption\",\n",
    "    col(\"AvgAmtPowerUse\") - coalesce(col(\"AvgAmtPowerReturnedToGrid\"), col(\"AvgAmtPowerUse\") * 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8911a6d7-780b-45a5-bfeb-dca292b65234",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "efficiency_df = net_power_df.withColumn(\n",
    "    \"Efficiency\",\n",
    "    col(\"AvgAmtProductOutput\") / col(\"AvgAmtNetPowerConsumption\")\n",
    ")\n",
    "\n",
    "efficiency_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59611b9b-245e-49d6-93d3-4f86608b3762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for the demo we override the table, in reality you would likely `append` new data\n",
    "\n",
    "efficiency_df.write.mode(\"overwrite\").saveAsTable(\"hive_metastore.default.greenefficiency\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "transform_green_manufacturing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
